# Updated Bayesian Thinking and Workflow for Political Science 

An aspirational project outline for a "post-PhD debrief" about Bayesian lessons learned. 
The idea here is to memorialize the [_shadow dissertation_](https://twitter.com/mikedecr/status/1281304803109412868), all of the things I learned about doing Bayesian statistics that are totally banal in the world of applied Bayesian researchers, but almost entirely absent from applied political science.
Political science has classic Bayesian texts (Jackman, Gill...) but they are a little dated.
A new generation of lessons for Bayesian research from statistics, biological sciences, and psychology is absent from political science except implicitly in individual papers.
Since these lessons come from other fields, papers and books that discuss the innovations unto themselves use different notational conventions and unfamiliar jargon, and the modeling intuitions aren't always easy to connect to political science applications.

The intent for this project is to center a conversation on new Bayesian thinking and methods in political science venues using political science problems as motivation.
It envisions two papers that discuss Bayesian approaches (broadly) separately from pragmatic advice for Bayesian implementation.

1. **Revisiting Bayesian Foundations in Political Science.**
   The "ideas" paper. 
   Reframing of Bayesian modeling ideas for a new era.
   We have new computational tools for estimating models, new insights about model parameterization, new insights about prior distributions, new models altogether, and so on.
   We also have new ways to talk about what Bayesian modeling _is_, new language about generative modeling, "Bayesian inference without frequentist language," and heuristics for talking about information without invoking hazy notions of "belief."
2. **Updating the Bayesian Modeling Workflow for Political Science**
   The "how-to" paper.
   Tricks of the trade for thinking about, building, and checking Bayesian models.
   Discuss software innovations since major Bayesian texts (Stan, HMC, VBI, ADVI) and how they affect model building, diagnosis, and validation (mainly focused on Stan, HMC, divergences, new Rhat...).
   New thinking about prior strategy: conjugacy v. entropy v. "objectivity". Implied priors.
   Parameterization (beta-binomial, non-centering...), regularizing priors (Normal/T/Cauchy), , covariance matrix priors (LKJ), Cholesky decomposition, sparsity-inducing priors, simplexes. 
   Workflow: prior checks, implied priors, posterior checking, LOO



